{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BPE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Byte-Pair Encoding**\n",
        "\n",
        "BPE is a simple form of data compression algorithm in which the most common pair of consecutive bytes of data is replaced with a byte that does not occur in that data"
      ],
      "metadata": {
        "id": "2jPzslh3G9nZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithm**\n",
        "\n",
        "1. Prepare a large enough training data (i.e. corpus)\n",
        "\n",
        "2. Define a desired subword vocabulary size\n",
        "\n",
        "3. Split word to sequence of characters and appending suffix “</w>” to end of word with word frequency. So the basic unit is character in this stage. For example, the frequency of “low” is 5, then we rephrase it to “l o w </w>”: 5\n",
        "\n",
        "4. Generating a new subword according to the high frequency occurrence.\n",
        "\n",
        "5. Repeating step 4 until reaching subword vocabulary size which is defined in step 2 or the next highest frequency pair is 1."
      ],
      "metadata": {
        "id": "3wWVovPo1tLe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vQ-RP3sGx4g"
      },
      "outputs": [],
      "source": [
        "import re, collections"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(vocab):\n",
        "  pairs = collections.defaultdict(int)\n",
        "  for word, freq in vocab.items():\n",
        "    symbols = word.split()\n",
        "    for i in range(len(symbols)-1):\n",
        "      pairs[symbols[i],  symbols[i+1]]+=freq\n",
        "  return pairs"
      ],
      "metadata": {
        "id": "PEStyTbzHSCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_vocab(pair, v_in):\n",
        "  v_out = {}\n",
        "  bigram = re.escape(\" \".join(pair))\n",
        "  p = re.compile(r'(?<!\\S)' + bigram + r\"(?!\\S)\")\n",
        "  for word in v_in:\n",
        "    w_out = p.sub(\"\".join(pair), word)\n",
        "    v_out[w_out] = v_in[word]\n",
        "  return v_out"
      ],
      "metadata": {
        "id": "pAuEnddqJyTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2, 'n e w e s t </w>' : 6, 'w i d e s t </w>' : 3}"
      ],
      "metadata": {
        "id": "SIv6RwPPM8VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_merges = 10\n",
        "for i in range(num_merges):\n",
        "  pairs = get_stats(vocab)\n",
        "  best = max(pairs, key = pairs.get)\n",
        "  vocal = merge_vocab(best, vocab)\n",
        "  print(best)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pdT7r_KL_hs",
        "outputId": "d0066cf5-9408-43b9-d977-8af85791fad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('e', 's')\n",
            "('e', 's')\n",
            "('e', 's')\n",
            "('e', 's')\n",
            "('e', 's')\n",
            "('e', 's')\n",
            "('e', 's')\n",
            "('e', 's')\n",
            "('e', 's')\n",
            "('e', 's')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Taking “low: 5”, “lower: 2”, “newest: 6” and “widest: 3” as an example, the highest frequency subword pair is e and s. It is because we get 6 count from newest and 3 count from widest. Then new subword (es) is formed and it will become a candidate in next iteration.\n",
        "\n",
        "In the second iteration, the next high frequency subword pair is es (generated from previous iteration )and t. It is because we get 6count from newest and 3 count from widest.\n",
        "\n",
        "Keep iterate until built a desire size of vocabulary size or the next highest frequency pair is 1."
      ],
      "metadata": {
        "id": "-WKFG2rAOFS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using Library**"
      ],
      "metadata": {
        "id": "0ldbVOt50N8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install bpe\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4N_ul8kPhf0",
        "outputId": "0a9bbd31-e416-4ca1-ff59-8242f8d29cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bpe\n",
            "  Downloading bpe-1.0-py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from bpe) (0.11.2)\n",
            "Collecting hypothesis\n",
            "  Downloading hypothesis-6.35.0-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bpe) (4.62.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from bpe) (3.2.5)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from bpe) (3.6.4)\n",
            "Collecting mypy\n",
            "  Downloading mypy-0.931-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (14.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.0 MB 47.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis->bpe) (21.2.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis->bpe) (2.4.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mypy->bpe) (1.2.2)\n",
            "Collecting typed-ast<2,>=1.4.0\n",
            "  Downloading typed_ast-1.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (843 kB)\n",
            "\u001b[K     |████████████████████████████████| 843 kB 46.4 MB/s \n",
            "\u001b[?25hCollecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.10 in /usr/local/lib/python3.7/dist-packages (from mypy->bpe) (3.10.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->bpe) (1.15.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->bpe) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->bpe) (8.12.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->bpe) (1.11.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->bpe) (1.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->bpe) (57.4.0)\n",
            "Installing collected packages: typed-ast, mypy-extensions, mypy, hypothesis, bpe\n",
            "Successfully installed bpe-1.0 hypothesis-6.35.0 mypy-0.931 mypy-extensions-0.4.3 typed-ast-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bpe import Encoder"
      ],
      "metadata": {
        "id": "tt00FkRANIYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Generated with http://pythonpsum.com\n",
        "test_corpus = '''\n",
        "    Object raspberrypi functools dict kwargs. Gevent raspberrypi functools. Dunder raspberrypi decorator dict didn't lambda zip import pyramid, she lambda iterate?\n",
        "    Kwargs raspberrypi diversity unit object gevent. Import fall integration decorator unit django yield functools twisted. Dunder integration decorator he she future. Python raspberrypi community pypy. Kwargs integration beautiful test reduce gil python closure. Gevent he integration generator fall test kwargs raise didn't visor he itertools...\n",
        "    Reduce integration coroutine bdfl he python. Cython didn't integration while beautiful list python didn't nit!\n",
        "    Object fall diversity 2to3 dunder script. Python fall for: integration exception dict kwargs dunder pycon. Import raspberrypi beautiful test import six web. Future integration mercurial self script web. Return raspberrypi community test she stable.\n",
        "    Django raspberrypi mercurial unit import yield raspberrypi visual rocksdahouse. Dunder raspberrypi mercurial list reduce class test scipy helmet zip?\n",
        "'''\n",
        "\n",
        "encoder = Encoder(200, pct_bpe=0.88)  # params chosen for demonstration purposes\n",
        "encoder.fit(test_corpus.split('\\n'))\n",
        "\n",
        "example = \"Vizzini: He didn't fall? INCONCEIVABLE!\"\n",
        "print(encoder.tokenize(example))\n",
        "print()\n",
        "# ['__sow', 'vi', 'z', 'zi', 'ni', '__eow', '__sow', ':', '__eow', 'he', 'didn', \"'\", 't', 'fall', '__sow', '?', '__eow', '__sow', 'in', 'co', 'n', 'ce', 'iv', 'ab', 'le', '__eow', '__sow', '!', '__eow']\n",
        "print(next(encoder.transform([example])))\n",
        "print()\n",
        "# [24, 108, 82, 83, 71, 25, 24, 154, 25, 14, 10, 11, 12, 13, 24, 85, 25, 24, 140, 59, 39, 157, 87, 165, 114, 25, 24, 148, 25]\n",
        "print(next(encoder.inverse_transform(encoder.transform([example]))))\n",
        "# vizzini : he didn ' t fall ? inconceivable !"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ3N4Tu2PYSf",
        "outputId": "a93eb453-8e97-40ee-cf94-6d0718c48be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__sow', 'vi', 'z', 'zi', 'ni', '__eow', '__sow', ':', '__eow', 'he', 'didn', \"'\", 't', 'fall', '__sow', '?', '__eow', '__sow', 'in', 'co', 'n', 'ce', 'iv', 'ab', 'le', '__eow', '__sow', '!', '__eow']\n",
            "\n",
            "[25, 108, 82, 83, 71, 24, 25, 154, 24, 14, 10, 11, 12, 13, 25, 85, 24, 25, 140, 59, 39, 157, 87, 165, 114, 24, 25, 148, 24]\n",
            "\n",
            "vizzini : he didn ' t fall ? inconceivable !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "neHvaT_ZQHUu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}