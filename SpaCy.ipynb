{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "SpaCy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK8h-ZR7OCiW"
      },
      "source": [
        "# What's spaCy?\n",
        "\n",
        "SpaCy is **free**, **open-source library** for advanced **Natural language processing**(NLP) in Python.\n",
        "\n",
        "Suppose you're working with a lot of text, you'll eventually want to know more about it. For example, what's it about? What does the words mean in the context? Who is doing what to whom? What products and compnaies are mentioned in the text? Which texts are simmilar to each other.\n",
        "\n",
        "spaCy is designed specifically for **production use** and helps you build applications that process and \"understand\" large volume of text. It can be used to build **information extraction** or **natural language processing** systems, or to pre-process text for **deep learning**.\n",
        "\n",
        "\n",
        "## What spaCy isn't?\n",
        "\n",
        "- First, **spaCy** isn't a platform or an \"API\". Unlike a platform, spaCy doesn't provide a software as a service or a web application. It’s an open-source library designed to help you build NLP applications, not a consumable service.\n",
        "\n",
        "- Second, **spaCy is not an out-of-the-box chat bot engine.** While spaCy can be used to power conversational applications, it’s not designed specifically for chat bots, and only provides the underlying text processing capabilities.\n",
        "\n",
        "- Third, **spaCy is not research software.** It’s built on the latest research, but it’s designed to get things done. This leads to fairly different design decisions than NLTK or CoreNLP, which were created as platforms for teaching and research. The main difference is that spaCy is integrated and opinionated. spaCy tries to avoid asking the user to choose between multiple algorithms that deliver equivalent functionality. Keeping the menu small lets spaCy deliver generally better performance and developer experience.\n",
        "\n",
        "- Fourth, **spaCy is not a company** It’s an open-source library.The company publishing spaCy and other software is called Explosion AI.\n",
        "\n",
        "## Installation\n",
        "\n",
        "spaCy is compatible with **64bit of Cython 2.7/3.5+** and runs on **Unix/Linux**, **macOS/OS X** and **Windows**. The latest version of spaCy is available over pip and conda.\n",
        "\n",
        " --> Installation with pip in Linux,Windows and macOs/OS X for both version of Python 2.7/3.5+\n",
        " \n",
        "     pip install -U spacy or pip install spacy\n",
        "     \n",
        " --> Installation with conda in Linux,Windows and macOs/OS X for both version of Python 2.7/3.5+\n",
        " \n",
        "     conda install -c conda-forge spacy\n",
        "\n",
        "## Features\n",
        "\n",
        "Here, you'll come across mentions of spaCy's features and capabilities. \n",
        "\n",
        "### Statistical models\n",
        "\n",
        "Some of spaCy's features works independently, other requires statistical models to be loaded, which enable spaCy to **predict**\n",
        "linguistic annotations-For example, whether a word is a verb or noun. spaCy currently offers statistical models for a variety of languages, which can be installed as individual Python modules. Models can differ in size, speed, memory usage, accuracy, and the data they include. The model you choose always depends upon your use cases and the texts you're working with. For a general use case, the small and the default models are always a good start. They typically include the following components:\n",
        "\n",
        "  - **Binary weights** for the part-of-speech tagger, dependency parser and named entity recognizer to predict those    annotations in context.\n",
        "  \n",
        "  - **Lexical entries** in the vocabulary, i.e. words and their context-independent attributes like the shape or spelling.\n",
        "  \n",
        "  - **Data files** like lemmatization rules and lookup tables.\n",
        "  - **Word vectors**,  i.e. multi-dimensional meaning representations of words that let you determine how similar they are to each other.\n",
        "  - **Configuration** options, like the language and processing pipeline settings, to put spaCy in the correct state when you load in the model.\n",
        "  \n",
        "  \n",
        "### Linguistic annotations\n",
        "\n",
        "spaCy provides a variety of linguistic annotations to give you **insights into a text’s grammatical structure.** This includes the word types, like the parts of speech, and how the words are related to each other. For example, if you’re analyzing text, it makes a huge difference whether a noun is the subject of a sentence, or the object – or whether “google” is used as a verb, or refers to the website or company in a specific context.\n",
        "\n",
        "Once you’ve [downloaded and installed](https://spacy.io/usage/models) a model, you can load it via spacy.load() This will return a *Language* object containing all components and data needed to process text. We usually call it *nlp* object on a string of text will return a processed *Doc* :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiBWQ9mROCiY",
        "outputId": "5ad189dd-830d-483c-a7de-16a113ea67be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "# https://spacy.io/usage/linguistic-features\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)\n",
        "\n",
        "# Text: The original word text.\n",
        "# Lemma: The base form of the word.\n",
        "# POS: The simple part-of-speech tag.\n",
        "# Tag: The detailed part-of-speech tag.\n",
        "# Dep: Syntactic dependency, i.e. the relation between tokens.\n",
        "# Shape: The word shape – capitalization, punctuation, digits.\n",
        "# is alpha: Is the token an alpha character?\n",
        "# is stop: Is the token part of a stop list, i.e. the most common words of the language?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple PROPN nsubj\n",
            "is AUX aux\n",
            "looking VERB ROOT\n",
            "at ADP prep\n",
            "buying VERB pcomp\n",
            "U.K. PROPN compound\n",
            "startup NOUN dobj\n",
            "for ADP prep\n",
            "$ SYM quantmod\n",
            "1 NUM compound\n",
            "billion NUM pobj\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE2TLx4gOCih"
      },
      "source": [
        "Even though a *Doc* is processed - e.g. split into individual words and annotated - it still hols **all information of the original text**, like a whitespace characters. You can always get the offset of a token into the original string, or reconstruct the original by joining the tokens and their trailing whitespace. This way, you’ll never lose any information when processing text with spaCy.\n",
        "\n",
        "## Tokenization\n",
        "\n",
        "During processing, spaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language. For example, punctuation at the end of a sentence should be split off – whereas “U.K.” should remain one token.  Each *Doc* consists of individual tokens, and we can iterate over them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjGN_nv0OCii",
        "outputId": "c6174815-254c-48e7-ab7d-a28814576c13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple\n",
            "is\n",
            "looking\n",
            "at\n",
            "buying\n",
            "U.K.\n",
            "startup\n",
            "for\n",
            "$\n",
            "1\n",
            "billion\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX2nmdraOCio"
      },
      "source": [
        "<img src=\".\\Images\\16.png\">\n",
        "\n",
        "First, the raw text is split on whitespace characters, similar to text.split(' '). Then, the tokenizer processes the text from left to right. On each substring, it performs two checks:\n",
        "\n",
        "**1. Does the substring match a tokenizer exception rule?**  For example, “don’t” does not contain whitespace, but should be split into two tokens, “do” and “n’t”, while “U.K.” should always remain one token. \n",
        "\n",
        "**2. Can a prefix, suffix or infix be split off?** For example punctuation like commas, periods, hyphens or quotes.\n",
        "\n",
        "If there’s a match, the rule is applied and the tokenizer continues its loop, starting with the newly split substrings. This way, spaCy can split **complex, nested tokens** like combinations of abbreviations and multiple punctuation marks.\n",
        "\n",
        "<img src=\".\\Images\\17.png\">\n",
        "\n",
        "While punctuation rules are usually pretty general, tokenizer exceptions strongly depend on the specifics of the individual language. This is why each available language has its own subclass like *English* or *German*, that loads in lists of hard-coded data and exception rules.\n",
        "\n",
        "\n",
        "## Part-of-speech(pos) tags and dependencies\n",
        "\n",
        "After tokenization, spaCy can parse and tag a given *Doc*. This is where the statistical model comes in, which enables spaCy to make a prediction of which tag or label most likely applies in this context. A model consists of binary data and is produced by showing a system enough examples for it to make predictions that generalize across the language – for example, a word following “the” in English is most likely a noun.\n",
        "\n",
        "Linguistic annotations are available as **Token** .Like many NLP libraries, spaCy encodes all strings to hash values to reduce memory usage and improve efficiency. So to get the readable string representation of an attribute, we need to add an underscore _ to its name:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKlcvqUpOCip",
        "outputId": "a9a947da-db64-41eb-844f-eb4dbae0dc88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Coronavirus: Delhi resident tests positive for coronavirus, total 31 people infected in India\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
        "            token.shape_, token.is_alpha, token.is_stop)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coronavirus Coronavirus PROPN NNP ROOT Xxxxx True False\n",
            ": : PUNCT : punct : False False\n",
            "Delhi Delhi PROPN NNP ROOT Xxxxx True False\n",
            "resident resident NOUN NN amod xxxx True False\n",
            "tests test NOUN NNS nsubj xxxx True False\n",
            "positive positive ADJ JJ amod xxxx True False\n",
            "for for ADP IN prep xxx True True\n",
            "coronavirus coronavirus PROPN NNP pobj xxxx True False\n",
            ", , PUNCT , punct , False False\n",
            "total total ADJ JJ ROOT xxxx True False\n",
            "31 31 NUM CD nummod dd False False\n",
            "people people NOUN NNS dobj xxxx True False\n",
            "infected infect VERB VBN acl xxxx True False\n",
            "in in ADP IN prep xx True True\n",
            "India India PROPN NNP pobj Xxxxx True False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk1GFBtyOCiv"
      },
      "source": [
        "Using spaCy’s built-in **displaCy** visualizer, here’s what our example sentence and its dependencies look like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD755AQeOCiw",
        "outputId": "c9dcdddf-3d6c-4091-ae37-003605b5e96e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Google, Apple crack down on fake coronavirus apps\")\n",
        "displacy.serve(doc, style=\"dep\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using the 'dep' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n",
            "Shutting down server on port 5000.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHsBOuuDOCi2"
      },
      "source": [
        "## Named Entities \n",
        "\n",
        "A named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn’t always work perfectly and might need some tuning later, depending on your use case.\n",
        "\n",
        "Named entities are available as the ents property of a Doc:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u27O7CSkOCi3",
        "outputId": "497f661b-0c32-4855-acec-092d6a18fdee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Coronavirus: Delhi resident tests positive for coronavirus, total 31 people infected in India\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Delhi 13 18 GPE\n",
            "31 66 68 CARDINAL\n",
            "India 88 93 GPE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcvDvnc4OCi9"
      },
      "source": [
        "## Visualizing the Named Entity recognizer\n",
        "\n",
        "The entity visualizer, *ent* , highlight named entities and their label in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDDMsX0qOCi-",
        "outputId": "2dd6696d-0a66-45db-805a-9d6353127c08"
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "text = \"Coronavirus: Delhi resident tests positive for coronavirus, total 31 people infected in India\"\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "displacy.serve(doc, style=\"ent\")\n",
        "# https://spacy.io/api/annotation#named-entities"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\Anaconda3\\lib\\runpy.py:193: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<!DOCTYPE html>\n",
              "<html lang=\"en\">\n",
              "    <head>\n",
              "        <title>displaCy</title>\n",
              "    </head>\n",
              "\n",
              "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
              "<figure style=\"margin-bottom: 6rem\">\n",
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Coronavirus: \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Delhi\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " resident tests positive for coronavirus, total \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    31\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " people infected in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    India\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              "</div>\n",
              "</figure>\n",
              "</body>\n",
              "</html>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using the 'ent' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKn-HTp6OCjD"
      },
      "source": [
        "## Words vector and similarity\n",
        "\n",
        "Similarity is determined by comparing word vectors or “word embeddings”, multi-dimensional meaning representations of a word.Word vectors can be generated using an algorithm like word2vec and usually look like this:\n",
        "\n",
        "__Important_note:__ To make them compact and fast, spaCy's small models(all the pacakages end with sm) **don't ship with the word vectors**, and only include context-sensitive tensors. This means you can still use the similarity() to compare documents, tokens and spans - but result won't be as good, and individual tokens won't have any vectors is assigned. So, in orders to use *real* word vectors, you need to download a larger model:\n",
        "\n",
        "     python -m spacy download en_core_web_md\n",
        "     \n",
        "Models that come with built-in word vectors make them available as the Token.vector attribute. Doc.vector and Span.vector will default to an average of their token vectors. You can also check if a token has a vector assigned, and get the L2 norm, which can be used to normalize vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6TSIhMGY5Fq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "10276a36-c937-4cbd-d0d6-ec137211ec10"
      },
      "source": [
        "# !python -m spacy download en_core_web_md\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_md\")\n",
        "import en_core_web_md\n",
        "nlp = en_core_web_md.load()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIvTxeUDOCjE",
        "outputId": "4604da26-d815-4eaa-813d-750045c01390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "tokens = nlp(\"lion bear apple banana fadsfdshds\")\n",
        "\n",
        "for token in tokens:\n",
        "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n",
        "# Vector norm: The L2 norm of the token’s vector (the square root of the sum of the values squared)\n",
        "# has vector: Does the token have a vector representation?\n",
        "# OOV: Out-of-vocabulary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lion True 6.5120897 False\n",
            "bear True 5.881604 False\n",
            "apple True 7.1346846 False\n",
            "banana True 6.700014 False\n",
            "fadsfdshds False 0.0 True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__DblcWGOCjK"
      },
      "source": [
        "The words “lion”, “bear”, “apple” and \"banana\" are all pretty common in English, so they’re part of the model’s vocabulary, and come with a vector. The word “fadsfdshds” on the other hand is a lot less common and out-of-vocabulary – so its vector representation consists of 300 dimensions of 0, which means it’s practically nonexistent. If your application will benefit from a large vocabulary with more vectors, you should consider using one of the larger models or loading in a full vector package, for example, *en_vectors_web_lg*, which includes over 1 million unique vectors.\n",
        "\n",
        "spaCy is able to compare two objects, and make a prediction of how similar they are. Predicting similarity is useful for building recommendation systems or flagging duplicates. For example, you can suggest a user content that’s similar to what they’re currently looking at, or label a support ticket as a duplicate if it’s very similar to an already existing one.\n",
        "\n",
        "Each Doc, Span and Token comes with a .similarity() method that lets you compare it with another object, and determine the similarity. Of course similarity is always subjective – whether “dog” and “cat” are similar really depends on how you’re looking at it. spaCy’s similarity model usually assumes a pretty general-purpose definition of similarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNCOLsJxOCjL",
        "outputId": "5a69ed2a-d7fa-44af-84c0-7bfe3cd0a675",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")  # make sure to use larger model!\n",
        "tokens = nlp(\"lion bear cow apple mango spinach\")\n",
        "\n",
        "for token11 in tokens:\n",
        "    for token13 in tokens:\n",
        "        print(token11.text, token13.text, token11.similarity(token13))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lion lion 1.0\n",
            "lion bear 0.6390859\n",
            "lion cow 0.4780627\n",
            "lion apple 0.33227408\n",
            "lion mango 0.21551447\n",
            "lion spinach 0.10201545\n",
            "bear lion 0.6390859\n",
            "bear bear 1.0\n",
            "bear cow 0.43222296\n",
            "bear apple 0.31760347\n",
            "bear mango 0.19838898\n",
            "bear spinach 0.125805\n",
            "cow lion 0.4780627\n",
            "cow bear 0.43222296\n",
            "cow cow 1.0\n",
            "cow apple 0.36605674\n",
            "cow mango 0.28737056\n",
            "cow spinach 0.318152\n",
            "apple lion 0.33227408\n",
            "apple bear 0.31760347\n",
            "apple cow 0.36605674\n",
            "apple apple 1.0\n",
            "apple mango 0.6165637\n",
            "apple spinach 0.43755493\n",
            "mango lion 0.21551447\n",
            "mango bear 0.19838898\n",
            "mango cow 0.28737056\n",
            "mango apple 0.6165637\n",
            "mango mango 1.0\n",
            "mango spinach 0.6105537\n",
            "spinach lion 0.10201545\n",
            "spinach bear 0.125805\n",
            "spinach cow 0.318152\n",
            "spinach apple 0.43755493\n",
            "spinach mango 0.6105537\n",
            "spinach spinach 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrGrKHiZOCjP"
      },
      "source": [
        "In the above case you can see that \"lion\" and \"bear\" have a similarity of 63%. Identical tokens are obviously 100% similar to each other(just not always exactly 1.0, because of vector math and floating point imprecisions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QNrLk4KOCjQ"
      },
      "source": [
        "## Pipelines\n",
        "\n",
        "When you call nlp on a text, spaCy first tokenizes the text to produce a Doc object. The Doc is then processed in several different steps – this is also referred to as the processing pipeline. The pipeline used by the default models consists of a tagger, a parser and an entity recognizer. Each pipeline component returns the processed Doc, which is then passed on to the next component.\n",
        "\n",
        "<img src=\".\\Images\\18.png\">\n",
        "<img src=\".\\Images\\19.png\">\n",
        "\n",
        "The processing pipeline always **depends on the statistical model** and its capabilities. For example, a pipeline can only include an entity recognizer component if the model includes data to make predictions of entity labels. This is why each model will specify the pipeline to use in its meta data, as a simple list containing the component names:\n",
        "\n",
        "     \"pipeline\": [\"tagger\", \"parser\", \"ner\"]\n",
        "     \n",
        "     \n",
        "## Vocab, hashes and lexemes\n",
        "\n",
        "Whenever possible, spaCy tries to store data in a vocabulary, the Vocab, that will be shared by multiple documents. To save memory, spaCy also encodes all strings to hash values – in this case for example, “coffee” has the hash 3197928453018144401. Entity labels like “ORG” and part-of-speech tags like “VERB” are also encoded. Internally, spaCy only “speaks” in hash values.\n",
        "\n",
        "<img src=\".\\Images\\20.png\">\n",
        "\n",
        "If you process lots of documents containing the word “coffee” in all kinds of different contexts, storing the exact string “coffee” every time would take up way too much space. So instead, spaCy hashes the string and stores it in the StringStore. You can think of the StringStore as a **lookup table that works in both directions** – you can look up a string to get its hash, or a hash to get its string:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfBJVYPFOCjR",
        "outputId": "0d1fcca7-1f3d-439c-8563-5202279f2546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I love coffee\")\n",
        "print(doc.vocab.strings[\"coffee\"])  # 3197928453018144401\n",
        "print(doc.vocab.strings[3197928453018144401])  # 'coffee'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3197928453018144401\n",
            "coffee\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOZTDZOBOCjV",
        "outputId": "bf147f54-5c35-478f-d1a5-d6a25ebb4a3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# Let's check with other words like 'tea'\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I love tea, over coffee\")\n",
        "print(doc.vocab.strings[\"tea\"]) # 6041671307218480733\n",
        "print(doc.vocab.strings[6041671307218480733])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6041671307218480733\n",
            "tea\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IIz6ZGVOCjb"
      },
      "source": [
        "Now that all strings are encoded, the entries in the vocabulary **don’t need to include the word text** themselves. Instead, they can look it up in the StringStore via its hash value. Each entry in the vocabulary, also called *Lexeme*, contains the context-independent information about a word. For example, no matter if “love” is used as a verb or a noun in some context, its spelling and whether it consists of alphabetic characters won’t ever change. Its hash value will also always be the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHgM-ZaaOCjb",
        "outputId": "d7ab18a0-a47f-47cc-a582-b874d9db89ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I love tea, over coffee!\")\n",
        "for word in doc:\n",
        "    lexeme = doc.vocab[word.text]\n",
        "    # print(lexeme)\n",
        "    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
        "            lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I 4690420944186131903 X I I True False True en\n",
            "love 3702023516439754181 xxxx l ove True False False en\n",
            "tea 6041671307218480733 xxx t tea True False False en\n",
            ", 2593208677638477497 , , , False False False en\n",
            "over 5456543204961066030 xxxx o ver True False False en\n",
            "coffee 3197928453018144401 xxxx c fee True False False en\n",
            "! 17494803046312582752 ! ! ! False False False en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEHgAO9hOCjf"
      },
      "source": [
        "The mapping of words to hashes doesn’t depend on any state. To make sure each value is unique, spaCy uses a hash function to calculate the hash based on the word string. This also means that the hash for “tea” will always be the same, no matter which model you’re using or how you’ve configured spaCy.\n",
        "\n",
        "However, hashes cannot be reversed and there’s no way to resolve 6041671307218480733 back to “tea”. All spaCy can do is look it up in the vocabulary. That’s why you always need to make sure all objects you create have access to the same vocabulary. If they don’t, spaCy might not be able to find the strings it needs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG5mWEBOOCjf",
        "outputId": "f40495ea-be13-4928-b11b-288fc34e95e1"
      },
      "source": [
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "from spacy.vocab import Vocab\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I love tea, over coffee\")  # Original Doc\n",
        "print(doc.vocab.strings[\"tea\"])  # 6041671307218480733\n",
        "print(doc.vocab.strings[6041671307218480733])  # 'tea' \n",
        "\n",
        "empty_doc = Doc(Vocab())  # New Doc with empty Vocab\n",
        "# empty_doc.vocab.strings[6041671307218480733] will raise an error :(\n",
        "\n",
        "empty_doc.vocab.strings.add(\"tea\")  # Add \"tea\" and generate hash\n",
        "print(empty_doc.vocab.strings[6041671307218480733])  # 'tea' \n",
        "\n",
        "new_doc = Doc(doc.vocab)  # Create new doc with first doc's vocab\n",
        "print(new_doc.vocab.strings[6041671307218480733])  # 'tea' 👍"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6041671307218480733\n",
            "tea\n",
            "tea\n",
            "tea\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZj2DWCHOCji"
      },
      "source": [
        "If the vocabulary doesn’t contain a string for 6041671307218480733, spaCy will raise an error. You can re-add “tea” manually, but this only works if you actually know that the document contains that word. To prevent this problem, spaCy will also export the Vocab when you save a Doc or nlp object. This will give you the object and its encoded annotations, plus the “key” to decode it.\n",
        "\n",
        "## Knowledge base\n",
        "\n",
        "To support the entity linking task, spaCy stores external knowledge in a KnowledgeBase. The knowledge base (KB) uses the Vocab to store its data efficiently.\n",
        "\n",
        "A knowledge base is created by first adding all entities to it. Next, for each potential mention or alias, a list of relevant KB IDs and their prior probabilities is added. The sum of these prior probabilities should never exceed 1 for any given alias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrKCh46yOCjj",
        "outputId": "76b86340-12ad-444e-e789-a6f1256b65ee"
      },
      "source": [
        "import spacy\n",
        "from spacy.kb import KnowledgeBase\n",
        "\n",
        "# load the model and create an empty KB\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=3)\n",
        "\n",
        "# adding entities\n",
        "kb.add_entity(entity=\"Q1004791\", freq=6, entity_vector=[0, 3, 5])\n",
        "kb.add_entity(entity=\"Q42\", freq=342, entity_vector=[1, 9, -3])\n",
        "kb.add_entity(entity=\"Q5301561\", freq=12, entity_vector=[-2, 4, 2])\n",
        "\n",
        "# adding aliases\n",
        "kb.add_alias(alias=\"Douglas\", entities=[\"Q1004791\", \"Q42\", \"Q5301561\"], probabilities=[0.6, 0.1, 0.2])\n",
        "kb.add_alias(alias=\"Douglas Adams\", entities=[\"Q42\"], probabilities=[0.9])\n",
        "\n",
        "print()\n",
        "print(\"Number of entities in KB:\",kb.get_size_entities()) # 3\n",
        "print(\"Number of aliases in KB:\", kb.get_size_aliases()) # 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Number of entities in KB: 3\n",
            "Number of aliases in KB: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-ooxNNMOCjm"
      },
      "source": [
        "## Candidate generation\n",
        "\n",
        "Given a textual entity, the Knowledge Base can provide a list of plausible candidates or entity identifiers. The EntityLinker will take this list of candidates as input, and disambiguate the mention to the most probable identifier, given the document context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXs4SckjOCjn",
        "outputId": "0ac0d208-fe43-4f22-cd34-39583e7b95a5"
      },
      "source": [
        "import spacy\n",
        "from spacy.kb import KnowledgeBase\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=3)\n",
        "\n",
        "# adding entities\n",
        "kb.add_entity(entity=\"Q1004791\", freq=6, entity_vector=[0, 3, 5])\n",
        "kb.add_entity(entity=\"Q42\", freq=342, entity_vector=[1, 9, -3])\n",
        "kb.add_entity(entity=\"Q5301561\", freq=12, entity_vector=[-2, 4, 2])\n",
        "\n",
        "# adding aliases\n",
        "kb.add_alias(alias=\"Douglas\", entities=[\"Q1004791\", \"Q42\", \"Q5301561\"], probabilities=[0.6, 0.1, 0.2])\n",
        "\n",
        "candidates = kb.get_candidates(\"Douglas\")\n",
        "for c in candidates:\n",
        "    print(\" \", c.entity_, c.prior_prob, c.entity_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Q1004791 0.6000000238418579 [0.0, 3.0, 5.0]\n",
            "  Q42 0.10000000149011612 [1.0, 9.0, -3.0]\n",
            "  Q5301561 0.20000000298023224 [-2.0, 4.0, 2.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpiDdwunOCjr"
      },
      "source": [
        "## Serialization\n",
        "\n",
        "If you’ve been modifying the pipeline, vocabulary, vectors and entities, or made updates to the model, you’ll eventually want to **save your progress** – for example, everything that’s in your *nlp* object. This means you’ll have to translate its contents and structure into a format that can be saved, like a file or a byte string. This process is called **<u>serialization</u>**. spaCy comes with **built-in serialization** methods and supports the Pickle protocol.\n",
        "\n",
        "All container classes, i.e. Language (nlp), Doc, Vocab and StringStore have the following methods available:\n",
        "<img src=\".\\Images\\21.png\">\n",
        "\n",
        "\n",
        "## Training\n",
        "\n",
        "spaCy’s models are statistical and every “decision” they make – for example, which part-of-speech tag to assign, or whether a word is a named entity – is a prediction. This prediction is based on the examples the model has seen during training. To train a model, you first need training data – examples of text, and the labels you want the model to predict. This could be a part-of-speech tag, a named entity or any other information.\n",
        "\n",
        "The model is then shown the unlabelled text and will make a prediction. Because we know the correct answer, we can give the model feedback on its prediction in the form of an **error gradient** of the **loss function** that calculates the difference between the training example and the expected output. The greater the difference, the more significant the gradient and the updates to our model.\n",
        "\n",
        "<img src=\".\\Images\\22.png\">\n",
        "\n",
        "When a training model, we don't want to memorize our examples- we want it to come up with theory that can be generalized with other examples. After all we don't just want the model to learn that this one instance of \"Amazon\" right here is a company - we want it to learn that \"Amazon\", in contexts like this, is most likely the company. That's why the training data should be representative always be the source of the data we want to process. A model is trained on wikipedia, where the sentences in the first person is to rare, will likely perform badly on Twitter.Similarly, model trained on romantic novels will likely perform bad in legal text.\n",
        "\n",
        "This also means that in order to know how the model is performing, and whether its learning the right things, you don't only need of training data- you'll also need evaluation data aswell. If you only test those data from which you trained, you will have no idea how well it's generalizing. If you want to train the model from scratch, you usually need at least 200 of images for training and evaluation. To update the existing model, you can achieve decent results with very few examples - as long as they're representative.\n",
        "\n",
        "\n",
        "## Language Data\n",
        "\n",
        "Every language is different from each other and usually full of exceptions and special cases, especially the amongst the most common words. Some of these are shared across languages, while others are entirely specific - usually it's to specific that they need to be hard-coded. The language module contains all the language-specific data, organized in simple Python files. This makes the data easy to update and extend.\n",
        "\n",
        "The shared language data in the directory root includes rules that can be generalized across languages – for example, rules for basic punctuation, emoji, emoticons, single-letter abbreviations and norms for equivalent tokens with different spellings, like \" and ”. This helps the models make more accurate predictions. The individual language data in a submodule contains rules that are only relevant to a particular language. It also takes care of putting together all components and creating the Language subclass – for example, English or German.\n",
        "\n",
        "<img src=\".\\Images\\23.png\">\n",
        "\n",
        "<img src=\".\\Images\\24.png\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vTN68FROCjs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}