{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "StanfordNLP 0.20.- Python NLP library for Many Human Language.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3Z-5EABeOYx"
      },
      "source": [
        "# StanfordNLP\n",
        "\n",
        "StanfordNLP is a Python natural language analysis package. It contains tools, which can be used in a pipeline, to convert a string containing human language text into lists of sentences and words, to generate base forms of those words, their parts of speech and morphological features, and to give a syntactic structure dependency parse, which is designed to be parallel among more than 70 languages, using the Universal Dependencies formalism. In addition, it is able to call the CoreNLP Java package and inherits additonal functionality from there, such as constituency parsing, coreference resolution, and linguistic pattern matching.\n",
        "\n",
        "This StanfordNLP package is built with highly accurate neural network components that enable efficient training and evaluation with your own annotated data. The modules are built on top of ***PyTorch***. You will get much faster performance if you run this system on a GPU-enabled machine. This package is a combination of software based on the Stanford entry in the CoNLL 2018 Shared Task on Universal Dependency Parsing, and the group’s official Python interface to the Java Stanford CoreNLP software. The CoNLL UD system is partly a cleaned up version of code used in the shared task and partly an approximate rewrite in PyTorch of the original Tensorflow version of the tagger and parser.\n",
        "\n",
        "\n",
        "# Installation & Model Downlaod\n",
        "\n",
        "### Installation\n",
        "\n",
        "For installing nlp run below command, always install StanfordNLP through [PyPi]('https://pypi.org/'), once installed run in your comand line or anaconda prompt\n",
        "\n",
        "    pip install stanfordnlp\n",
        "    \n",
        "This will take care of all your necessary dependencies to run StanfordNLP. The neural pipeline of StanfordNLP depends on PyTorch 1.0.0 or a later version with compatible APIs.\n",
        "\n",
        "**Note:** Installation in PyTorch.\n",
        "\n",
        "For Conda(Works fine for Windows and Linux), \n",
        "\n",
        "    conda install pytorch torchvision cpuonly -c pytorch\n",
        "    \n",
        "Conda(Mac)\n",
        "\n",
        "    conda install pytorch torchvision -c pytorch\n",
        "    \n",
        "For Pip(Windows and Linux)\n",
        "\n",
        "    pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
        "    \n",
        "Pip(Mac)\n",
        "\n",
        "    pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3X04c0PnHlj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "ad0dbe03-a03f-47a9-adb8-c195921482b6"
      },
      "source": [
        "!pip install stanfordnlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stanfordnlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/bf/5d2898febb6e993fcccd90484cba3c46353658511a41430012e901824e94/stanfordnlp-0.2.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (4.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (2.21.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.18.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (3.10.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2020.4.5.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (46.1.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (1.12.0)\n",
            "Installing collected packages: stanfordnlp\n",
            "Successfully installed stanfordnlp-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh-cnWY9nAYA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "outputId": "e680f12f-0329-47f2-fb42-d3e181a06667"
      },
      "source": [
        "!pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.4.0+cpu\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cpu/torch-1.4.0%2Bcpu-cp36-cp36m-linux_x86_64.whl (127.2MB)\n",
            "\u001b[K     |████████████████████████████████| 127.2MB 94kB/s \n",
            "\u001b[?25hCollecting torchvision==0.5.0+cpu\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.5.0%2Bcpu-cp36-cp36m-linux_x86_64.whl (5.4MB)\n",
            "\u001b[K     |████████████████████████████████| 5.4MB 30.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0+cpu) (1.18.2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0+cpu) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0+cpu) (1.12.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "  Found existing installation: torchvision 0.5.0\n",
            "    Uninstalling torchvision-0.5.0:\n",
            "      Successfully uninstalled torchvision-0.5.0\n",
            "Successfully installed torch-1.4.0+cpu torchvision-0.5.0+cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlY5aWKfeOYz",
        "outputId": "624f24e8-ced1-4d8f-da4a-e5e2b178ff0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 891
        }
      },
      "source": [
        "import stanfordnlp\n",
        "stanfordnlp.download('en')   # This downloads the English models for the neural pipeline\n",
        "nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English\n",
        "doc = nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\n",
        "doc.sentences[0].print_dependencies()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the default treebank \"en_ewt\" for language \"en\".\n",
            "Would you like to download the models for: en_ewt now? (Y/n)\n",
            "Y\n",
            "\n",
            "Default download directory: /root/stanfordnlp_resources\n",
            "Hit enter to continue or type an alternate directory.\n",
            "\n",
            "\n",
            "Downloading models for: en_ewt\n",
            "Download location: /root/stanfordnlp_resources/en_ewt_models.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235M/235M [00:23<00:00, 10.1MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Download complete.  Models saved to: /root/stanfordnlp_resources/en_ewt_models.zip\n",
            "Extracting models file for: en_ewt\n",
            "Cleaning up...Done.\n",
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: pos\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: lemma\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Building an attentional Seq2Seq model...\n",
            "Using a Bi-LSTM encoder\n",
            "Using soft attention for LSTM.\n",
            "Finetune all embeddings.\n",
            "[Running seq2seq lemmatizer with edit classifier]\n",
            "---\n",
            "Loading: depparse\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n",
            "('Barack', '4', 'nsubj:pass')\n",
            "('Obama', '1', 'flat')\n",
            "('was', '4', 'aux:pass')\n",
            "('born', '0', 'root')\n",
            "('in', '6', 'case')\n",
            "('Hawaii', '4', 'obl')\n",
            "('.', '4', 'punct')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frJ2uu71eOY8"
      },
      "source": [
        "The last command here will print out the words in the first sentence in the input string (or Document, as it is represented in StanfordNLP), as well as the indices for the word that governs it in the Universal Dependencies parse of that sentence (its “head”), along with the dependency relation between the words.\n",
        "\n",
        "# Models for Human Languages\n",
        "\n",
        "Downloading a language pack is as simple as"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfdp_ERdeOY9"
      },
      "source": [
        "import stanfordnlp \n",
        "#stanfordnlp.download('ar')    # replace \"ar\" with the language"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_bZaxQ6eOZD"
      },
      "source": [
        "To use default langauge pack for any language, simply build the pipeline as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sivBs4M6eOZE",
        "outputId": "ae0a4a06-3cf0-495d-facd-6841847d8c9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        }
      },
      "source": [
        "import stanfordnlp \n",
        "nlp = stanfordnlp.Pipeline(lang=\"en\") # This sets up a default neural pipeline in English\n",
        "doc = nlp(\"Narendra Modi was born in India. He became Prime minister in 2014.\")\n",
        "doc.sentences[0].print_dependencies()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: pos\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: lemma\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Building an attentional Seq2Seq model...\n",
            "Using a Bi-LSTM encoder\n",
            "Using soft attention for LSTM.\n",
            "Finetune all embeddings.\n",
            "[Running seq2seq lemmatizer with edit classifier]\n",
            "---\n",
            "Loading: depparse\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n",
            "('Narendra', '4', 'nsubj:pass')\n",
            "('Modi', '1', 'flat')\n",
            "('was', '4', 'aux:pass')\n",
            "('born', '0', 'root')\n",
            "('in', '6', 'case')\n",
            "('India', '4', 'obl')\n",
            "('.', '4', 'punct')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxkg21YCeOZI"
      },
      "source": [
        "# Pipeline\n",
        "\n",
        "Users of StanfordNLP can process documents by building a Pipeline with the desired Processor units. The pipeline takes in a Document object or raw text, runs the processors in succession, and returns an annotated Document.\n",
        "\n",
        "## Options\n",
        "![alt text](1.png \"Title\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWChudrxeOZK",
        "outputId": "044aac77-d7de-4a19-c0b7-03f79deda69c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "import stanfordnlp\n",
        "\n",
        "MODELS_DIR = '.'\n",
        "stanfordnlp.download('en', MODELS_DIR) # Download the English models\n",
        "nlp = stanfordnlp.Pipeline(processors='tokenize,pos', models_dir=MODELS_DIR, treebank='en_ewt', use_gpu=True, pos_batch_size=3000) # Build the pipeline, specify part-of-speech processor's batch size\n",
        "doc = nlp(\"Barack Obama was born in Hawaii.\") # Run the pipeline on input text\n",
        "doc.sentences[0].print_tokens() # Look at the result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the default treebank \"en_ewt\" for language \"en\".\n",
            "Would you like to download the models for: en_ewt now? (Y/n)\n",
            "y\n",
            "\n",
            "Downloading models for: en_ewt\n",
            "Download location: ./en_ewt_models.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235M/235M [00:10<00:00, 22.2MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Download complete.  Models saved to: ./en_ewt_models.zip\n",
            "Extracting models file for: en_ewt\n",
            "Cleaning up...Done.\n",
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': './en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: pos\n",
            "With settings: \n",
            "{'model_path': './en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': './en_ewt_models/en_ewt.pretrain.pt', 'batch_size': 3000, 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n",
            "<Token index=1;words=[<Word index=1;text=Barack;upos=PROPN;xpos=NNP;feats=Number=Sing>]>\n",
            "<Token index=2;words=[<Word index=2;text=Obama;upos=PROPN;xpos=NNP;feats=Number=Sing>]>\n",
            "<Token index=3;words=[<Word index=3;text=was;upos=AUX;xpos=VBD;feats=Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin>]>\n",
            "<Token index=4;words=[<Word index=4;text=born;upos=VERB;xpos=VBN;feats=Tense=Past|VerbForm=Part|Voice=Pass>]>\n",
            "<Token index=5;words=[<Word index=5;text=in;upos=ADP;xpos=IN;feats=_>]>\n",
            "<Token index=6;words=[<Word index=6;text=Hawaii;upos=PROPN;xpos=NNP;feats=Number=Sing>]>\n",
            "<Token index=7;words=[<Word index=7;text=.;upos=PUNCT;xpos=.;feats=_>]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkWOk2bZeOZP"
      },
      "source": [
        "## Processors Summary\n",
        "\n",
        "Processors are units of the neural pipeline that create different annotations for a Document. The neural pipeline now supports the following processors:\n",
        "![alt text](2.png \"Title\")\n",
        "\n",
        "## Data Objects\n",
        "\n",
        "This section will describes the data objects used in StanfordNLP, and how they interact with each other.\n",
        "\n",
        "### Document\n",
        "\n",
        "A Document object holds the annotation of an entire document, and is automatically generated when a string is annotated by the Pipeline. It holds a collection of Sentences, and can be seamlessly translated into a CoNLL-U file.\n",
        "\n",
        "Objects of this class expose useful properties such as text, sentences, and conll_file.\n",
        "\n",
        "### Sentence\n",
        "\n",
        "A Sentence object represents a sentence (as is predicted by the tokenizer), and holds a list of the Tokens in the sentence, as well as a list of all its Words. It also processes the dependency parse as is predicted by the parser, through its member method build_dependencies.\n",
        "\n",
        "Objects of this class expose useful properties such as words, tokens, and dependencies, as well as methods such as print_tokens, print_words, print_dependencies.\n",
        "\n",
        "### Token\n",
        "\n",
        "A Token object holds a token, and a list of its underlying words. In the event that the token is a multi-word token (e.g., French au = à le), the token will have a range index as described in the CoNLL-U format specifications (e.g., 3-4), with its word property containing the underlying Words. In other cases, the Token object will be a simple wrapper around one Word object, where its words property is a singleton.\n",
        "\n",
        "### Word\n",
        "\n",
        "A Word object holds a syntactic word and all of its word-level annotations. In the example of multi-word tokens(MWT), these are generated as a result of multi-word token expansion, and are used in all downstream syntactic analyses such as tagging, lemmatization, and parsing. If a Word is the result from an MWT expansion, its text will usually not be found in the input raw text. Aside from multi-word tokens, Words should be similar to the familiar “tokens” one would see elsewhere.\n",
        "\n",
        "## TokenizeProcessor\n",
        "\n",
        "### Description\n",
        "Tokenizes the text and performs sentence segmentation.\n",
        "![alt text](3.png \"Title\")\n",
        "\n",
        "### Options\n",
        "![alt text](4.png \"Title\")\n",
        "\n",
        "### Example \n",
        "\n",
        "The tokenize processor is usually the first processor used in the pipeline. It performs tokenization and sentence segmentation at the same time. After this processor is run, the input document will become a list of Sentences. The list of tokens for sentence sent can then be accessed with sent.tokens. The code below shows an example of tokenization and sentence segmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiZAojsoeOZQ",
        "outputId": "fa915132-2e8b-4096-9a0c-e43539e6f273"
      },
      "source": [
        "import stanfordnlp\n",
        "\n",
        "nlp = stanfordnlp.Pipeline(processors='tokenize', lang='en')\n",
        "doc = nlp(\"This is a test sentence for stanfordnlp. This is another sentence.\")\n",
        "for i, sentence in enumerate(doc.sentences):\n",
        "    print(f\"====== Sentence {i+1} tokens =======\")\n",
        "    print(*[f\"index: {token.index.rjust(3)}\\ttoken: {token.text}\" for token in sentence.tokens], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': 'C:\\\\Users\\\\sudha\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n",
            "====== Sentence 1 tokens =======\n",
            "index:   1\ttoken: This\n",
            "index:   2\ttoken: is\n",
            "index:   3\ttoken: a\n",
            "index:   4\ttoken: test\n",
            "index:   5\ttoken: sentence\n",
            "index:   6\ttoken: for\n",
            "index:   7\ttoken: stanfordnlp\n",
            "index:   8\ttoken: .\n",
            "====== Sentence 2 tokens =======\n",
            "index:   1\ttoken: This\n",
            "index:   2\ttoken: is\n",
            "index:   3\ttoken: another\n",
            "index:   4\ttoken: sentence\n",
            "index:   5\ttoken: .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAleMxXBeOZV"
      },
      "source": [
        "## MWTProcessor\n",
        "\n",
        "### Description\n",
        "Expands multi-word tokens(MWT) predicted by the tokenizer.\n",
        "![alt text](5.png \"Title\")\n",
        "### Options\n",
        "![alt text](6.png \"Title\")\n",
        "### Example\n",
        "The mwt processor only requires tokenize. After these two processors have run, the Sentences will have lists of tokens and corresponding words based on the multi-word-token expander model. The list of tokens for sentence sent can be accessed with sent.tokens. The list of words for sentence sent can be accessed with sent.words. The list of words for a token token can be accessed with token.words. The code below shows an example of accessing tokens and words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fFTeVaWeOZW",
        "outputId": "60bcaa2a-b381-4a39-99a3-1abf94c546f9"
      },
      "source": [
        "import stanfordnlp\n",
        "stanfordnlp.download('fr') \n",
        "nlp = stanfordnlp.Pipeline(processors='tokenize,mwt', lang='fr')\n",
        "doc = nlp(\"Alors encore inconnu du grand public, Emmanuel Macron devient en 2014 ministre de l'Économie, de l'Industrie et du Numérique.\")\n",
        "print(*[f'token: {token.text.ljust(9)}\\t\\twords: {token.words}' for sent in doc.sentences for token in sent.tokens], sep='\\n')\n",
        "print('')\n",
        "print(*[f'word: {word.text.ljust(9)}\\t\\ttoken parent:{word.parent_token.index+\"-\"+word.parent_token.text}' for sent in doc.sentences for word in sent.words], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the default treebank \"fr_gsd\" for language \"fr\".\n",
            "Would you like to download the models for: fr_gsd now? (Y/n)\n",
            "Y\n",
            "\n",
            "Default download directory: C:\\Users\\sudha\\stanfordnlp_resources\n",
            "Hit enter to continue or type an alternate directory.\n",
            "\n",
            "\n",
            "Downloading models for: fr_gsd\n",
            "Download location: C:\\Users\\sudha\\stanfordnlp_resources\\fr_gsd_models.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████████████████| 235M/235M [09:54<00:00, 418kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Download complete.  Models saved to: C:\\Users\\sudha\\stanfordnlp_resources\\fr_gsd_models.zip\n",
            "Extracting models file for: fr_gsd\n",
            "Cleaning up...Done.\n",
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': 'C:\\\\Users\\\\sudha\\\\stanfordnlp_resources\\\\fr_gsd_models\\\\fr_gsd_tokenizer.pt', 'lang': 'fr', 'shorthand': 'fr_gsd', 'mode': 'predict'}\n",
            "---\n",
            "Loading: mwt\n",
            "With settings: \n",
            "{'model_path': 'C:\\\\Users\\\\sudha\\\\stanfordnlp_resources\\\\fr_gsd_models\\\\fr_gsd_mwt_expander.pt', 'lang': 'fr', 'shorthand': 'fr_gsd', 'mode': 'predict'}\n",
            "Building an attentional Seq2Seq model...\n",
            "Using a Bi-LSTM encoder\n",
            "Using soft attention for LSTM.\n",
            "Finetune all embeddings.\n",
            "Done loading processors!\n",
            "---\n",
            "token: Alors    \t\twords: [<Word index=1;text=Alors>]\n",
            "token: encore   \t\twords: [<Word index=2;text=encore>]\n",
            "token: inconnu  \t\twords: [<Word index=3;text=inconnu>]\n",
            "token: du       \t\twords: [<Word index=4;text=de>, <Word index=5;text=le>]\n",
            "token: grand    \t\twords: [<Word index=6;text=grand>]\n",
            "token: public   \t\twords: [<Word index=7;text=public>]\n",
            "token: ,        \t\twords: [<Word index=8;text=,>]\n",
            "token: Emmanuel \t\twords: [<Word index=9;text=Emmanuel>]\n",
            "token: Macron   \t\twords: [<Word index=10;text=Macron>]\n",
            "token: devient  \t\twords: [<Word index=11;text=devient>]\n",
            "token: en       \t\twords: [<Word index=12;text=en>]\n",
            "token: 2014     \t\twords: [<Word index=13;text=2014>]\n",
            "token: ministre \t\twords: [<Word index=14;text=ministre>]\n",
            "token: de       \t\twords: [<Word index=15;text=de>]\n",
            "token: l'       \t\twords: [<Word index=16;text=l'>]\n",
            "token: Économie \t\twords: [<Word index=17;text=Économie>]\n",
            "token: ,        \t\twords: [<Word index=18;text=,>]\n",
            "token: de       \t\twords: [<Word index=19;text=de>]\n",
            "token: l'       \t\twords: [<Word index=20;text=l'>]\n",
            "token: Industrie\t\twords: [<Word index=21;text=Industrie>]\n",
            "token: et       \t\twords: [<Word index=22;text=et>]\n",
            "token: du       \t\twords: [<Word index=23;text=de>, <Word index=24;text=le>]\n",
            "token: Numérique\t\twords: [<Word index=25;text=Numérique>]\n",
            "token: .        \t\twords: [<Word index=26;text=.>]\n",
            "\n",
            "word: Alors    \t\ttoken parent:1-Alors\n",
            "word: encore   \t\ttoken parent:2-encore\n",
            "word: inconnu  \t\ttoken parent:3-inconnu\n",
            "word: de       \t\ttoken parent:4-5-du\n",
            "word: le       \t\ttoken parent:4-5-du\n",
            "word: grand    \t\ttoken parent:6-grand\n",
            "word: public   \t\ttoken parent:7-public\n",
            "word: ,        \t\ttoken parent:8-,\n",
            "word: Emmanuel \t\ttoken parent:9-Emmanuel\n",
            "word: Macron   \t\ttoken parent:10-Macron\n",
            "word: devient  \t\ttoken parent:11-devient\n",
            "word: en       \t\ttoken parent:12-en\n",
            "word: 2014     \t\ttoken parent:13-2014\n",
            "word: ministre \t\ttoken parent:14-ministre\n",
            "word: de       \t\ttoken parent:15-de\n",
            "word: l'       \t\ttoken parent:16-l'\n",
            "word: Économie \t\ttoken parent:17-Économie\n",
            "word: ,        \t\ttoken parent:18-,\n",
            "word: de       \t\ttoken parent:19-de\n",
            "word: l'       \t\ttoken parent:20-l'\n",
            "word: Industrie\t\ttoken parent:21-Industrie\n",
            "word: et       \t\ttoken parent:22-et\n",
            "word: de       \t\ttoken parent:23-24-du\n",
            "word: le       \t\ttoken parent:23-24-du\n",
            "word: Numérique\t\ttoken parent:25-Numérique\n",
            "word: .        \t\ttoken parent:26-.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lPZGPhbeOZc"
      },
      "source": [
        "## POSProcessor\n",
        "\n",
        "### Description\n",
        "Labels tokens with their universal POS (UPOS) tags, treebank-specific POS (XPOS) tags, and universal morphological features (UFeats).\n",
        "![alt text](7.png \"Title\")\n",
        "### Options\n",
        "![alt text](8.png \"Title\")\n",
        "\n",
        "### Example\n",
        "\n",
        "Running the part of speech tagger simply requires tokenization and multi-word expansion. So the pipeline can be run with tokenize,mwt,pos as the list of processors. After the pipeline is run, the document will contain a list of sentences, and the sentences will contain lists of words. The part-of-speech tags can be accessed via the upos and xpos fields."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brB3VBLceOZd",
        "outputId": "c54cd1a2-be7d-4215-fbfb-6ab127cd68a3"
      },
      "source": [
        "import stanfordnlp\n",
        "\n",
        "nlp = stanfordnlp.Pipeline(processors='tokenize,mwt,pos')\n",
        "doc = nlp(\"Barack Obama was born in Hawaii.\")\n",
        "print(*[f'word: {word.text+\" \"}\\tupos: {word.upos}\\txpos: {word.xpos}' for sent in doc.sentences for word in sent.words], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': 'C:\\\\Users\\\\sudha\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: pos\n",
            "With settings: \n",
            "{'model_path': 'C:\\\\Users\\\\sudha\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\sudha\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n",
            "word: Barack \tupos: PROPN\txpos: NNP\n",
            "word: Obama \tupos: PROPN\txpos: NNP\n",
            "word: was \tupos: AUX\txpos: VBD\n",
            "word: born \tupos: VERB\txpos: VBN\n",
            "word: in \tupos: ADP\txpos: IN\n",
            "word: Hawaii \tupos: PROPN\txpos: NNP\n",
            "word: . \tupos: PUNCT\txpos: .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8TMzjoEeOZj"
      },
      "source": [
        "## LemmaProcessor\n",
        "\n",
        "### Description\n",
        "Generates the word lemmas for all tokens in the corpus.\n",
        "![alt text](9.png \"Title\")\n",
        "### Options\n",
        "![alt text](10.png \"Title\")\n",
        "### Example\n",
        "If your main interest is lemmatizing, you can supply a smaller processors list with just the prerequisites for lemma. After the pipeline is run, the document will contain a list of sentences, and the sentences will contain lists of words. The lemma information can be found in the lemma field."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKg1RfNEeOZj",
        "outputId": "76fc914b-4fbb-4f4e-a46f-7cd3fddcceb2"
      },
      "source": [
        "import stanfordnlp\n",
        "\n",
        "nlp = stanfordnlp.Pipeline(processors='tokenize,mwt,pos,lemma')\n",
        "doc = nlp(\"Barack Obama was born in Hawaii.\")\n",
        "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': 'C:\\\\Users\\\\sudha\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: pos\n",
            "With settings: \n",
            "{'model_path': 'C:\\\\Users\\\\sudha\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\sudha\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: lemma\n",
            "With settings: \n",
            "{'model_path': 'C:\\\\Users\\\\sudha\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Building an attentional Seq2Seq model...\n",
            "Using a Bi-LSTM encoder\n",
            "Using soft attention for LSTM.\n",
            "Finetune all embeddings.\n",
            "[Running seq2seq lemmatizer with edit classifier]\n",
            "Done loading processors!\n",
            "---\n",
            "word: Barack \tlemma: Barack\n",
            "word: Obama \tlemma: Obama\n",
            "word: was \tlemma: be\n",
            "word: born \tlemma: bear\n",
            "word: in \tlemma: in\n",
            "word: Hawaii \tlemma: Hawaii\n",
            "word: . \tlemma: .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpfqolfDeOZo"
      },
      "source": [
        "## DepparseProcessor\n",
        "\n",
        "### Description\n",
        "Provides an accurate syntactic dependency parser.\n",
        "![alt text](11.png \"Title\")\n",
        "### Options\n",
        "![alt text](12.png \"Title\")\n",
        "### Example\n",
        "The depparse processor depends on tokenize, mwt, pos, and lemma. After all these processors have been run, each Sentence in the output would have been parsed into Universal Dependencies structure, where the governor index of each word can be accessed by word.governor, and the dependency relation between the words word.dependency_relation. Note that the governor index starts at 1 for actual words, and is 0 only when the word itself is the root of the tree. This index should be offset by 1 when looking for the govenor word in the sentence. Here is an example to access dependency parse information:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OWd5JXgeOZp",
        "outputId": "b7349cd9-e816-4b3a-e331-85fab06b663c"
      },
      "source": [
        "import stanfordnlp\n",
        "\n",
        "nlp = stanfordnlp.Pipeline(processors='tokenize,mwt,pos,lemma,depparse', lang='fr')\n",
        "doc = nlp(\"Van Gogh grandit au sein d'une famille de l'ancienne bourgeoisie.\")\n",
        "print(*[f\"index: {word.index.rjust(2)}\\tword: {word.text.ljust(11)}\\tgovernor index: {word.governor}\\tgovernor: {(doc.sentences[0].words[word.governor-1].text if word.governor > 0 else 'root').ljust(11)}\\tdeprel: {word.dependency_relation}\" for word in doc.sentences[0].words], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': 'C:\\\\Users\\\\sudha\\\\stanfordnlp_resources\\\\fr_gsd_models\\\\fr_gsd_tokenizer.pt', 'lang': 'fr', 'shorthand': 'fr_gsd', 'mode': 'predict'}\n",
            "---\n",
            "Loading: mwt\n",
            "With settings: \n",
            "{'model_path': 'C:\\\\Users\\\\sudha\\\\stanfordnlp_resources\\\\fr_gsd_models\\\\fr_gsd_mwt_expander.pt', 'lang': 'fr', 'shorthand': 'fr_gsd', 'mode': 'predict'}\n",
            "Building an attentional Seq2Seq model...\n",
            "Using a Bi-LSTM encoder\n",
            "Using soft attention for LSTM.\n",
            "Finetune all embeddings.\n",
            "---\n",
            "Loading: pos\n",
            "With settings: \n",
            "{'model_path': 'C:\\\\Users\\\\sudha\\\\stanfordnlp_resources\\\\fr_gsd_models\\\\fr_gsd_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\sudha\\\\stanfordnlp_resources\\\\fr_gsd_models\\\\fr_gsd.pretrain.pt', 'lang': 'fr', 'shorthand': 'fr_gsd', 'mode': 'predict'}\n",
            "---\n",
            "Loading: lemma\n",
            "With settings: \n",
            "{'model_path': 'C:\\\\Users\\\\sudha\\\\stanfordnlp_resources\\\\fr_gsd_models\\\\fr_gsd_lemmatizer.pt', 'lang': 'fr', 'shorthand': 'fr_gsd', 'mode': 'predict'}\n",
            "Building an attentional Seq2Seq model...\n",
            "Using a Bi-LSTM encoder\n",
            "Using soft attention for LSTM.\n",
            "Finetune all embeddings.\n",
            "[Running seq2seq lemmatizer with edit classifier]\n",
            "---\n",
            "Loading: depparse\n",
            "With settings: \n",
            "{'model_path': 'C:\\\\Users\\\\sudha\\\\stanfordnlp_resources\\\\fr_gsd_models\\\\fr_gsd_parser.pt', 'pretrain_path': 'C:\\\\Users\\\\sudha\\\\stanfordnlp_resources\\\\fr_gsd_models\\\\fr_gsd.pretrain.pt', 'lang': 'fr', 'shorthand': 'fr_gsd', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "..\\aten\\src\\ATen\\native\\LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "index:  1\tword: Van        \tgovernor index: 3\tgovernor: grandit    \tdeprel: nsubj\n",
            "index:  2\tword: Gogh       \tgovernor index: 1\tgovernor: Van        \tdeprel: flat:name\n",
            "index:  3\tword: grandit    \tgovernor index: 0\tgovernor: root       \tdeprel: root\n",
            "index:  4\tword: à          \tgovernor index: 6\tgovernor: sein       \tdeprel: case\n",
            "index:  5\tword: le         \tgovernor index: 6\tgovernor: sein       \tdeprel: det\n",
            "index:  6\tword: sein       \tgovernor index: 3\tgovernor: grandit    \tdeprel: obl\n",
            "index:  7\tword: d'         \tgovernor index: 9\tgovernor: famille    \tdeprel: case\n",
            "index:  8\tword: une        \tgovernor index: 9\tgovernor: famille    \tdeprel: det\n",
            "index:  9\tword: famille    \tgovernor index: 6\tgovernor: sein       \tdeprel: nmod\n",
            "index: 10\tword: de         \tgovernor index: 13\tgovernor: bourgeoisie\tdeprel: case\n",
            "index: 11\tword: l'         \tgovernor index: 13\tgovernor: bourgeoisie\tdeprel: det\n",
            "index: 12\tword: ancienne   \tgovernor index: 13\tgovernor: bourgeoisie\tdeprel: amod\n",
            "index: 13\tword: bourgeoisie\tgovernor index: 9\tgovernor: famille    \tdeprel: nmod\n",
            "index: 14\tword: .          \tgovernor index: 3\tgovernor: grandit    \tdeprel: punct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfnnq0GseOZv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}